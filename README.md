# Attention Mechanism on OPUS Dataset

This repository implements the **core Attention mechanism**, the foundation behind all modern Transformer-based models like BERT, GPT, and llama. It is trained on a small subset of the **OPUS Books parallel corpus** and it is recommended to try to train your model on any open datasets and then **visualize the attention weights**, making the model both more interpretable and easier to understand.

---

## Why This Project?

After studying the paper ["Attention is All You Need"](https://arxiv.org/abs/1706.03762), I wanted to truly understand how  how attention works **in real time**. Instead of relying on libraries or prebuilt models, I took inspiration from Umar Jamil's videos and built a functional attention module for my own understanding.

This project was built to:
- Gain deep intuition about the mechanics of attention
- Learn how to move from **equations to working code**
- Create something that can visualize how attention operates on real language data

---

## Learnings 

 -   Built attention from first principles
 -   Understood how queries, keys, and values interact
 -   Gained experience with Hugging Face datasets, PyTorch, and tokenization

## Future Plans
- Read more papers and try to implement them from the ground up.
- Learn more about RL/Deep-RL, Diffusion Models and implement them on real world assets for testing.
- Teach people about complex topics across domains like Computer Architecture, Networking, Deep Learning etc.
